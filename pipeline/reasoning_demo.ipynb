{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c37b11",
   "metadata": {},
   "source": [
    "# Reasoning Direction\n",
    "\n",
    "This notebook aims to estimate the \"reasoning\" direction within the LLM activation space.\n",
    "We're basing this approach on the methodology used to find the \"refusal\" direction, but with a key difference:\n",
    "\n",
    "- **Refusal paper approach**: Used 1 LLM with 2 types of prompts (harmful vs harmless)\n",
    "- **Our approach**: Use 2 models (original vs reasoning-tuned) with the same prompts (GSM8K math problems)\n",
    "\n",
    "We'll collect activations from both models, calculate the difference (reasoning direction),\n",
    "and then test if adding this direction to the non-reasoning model enhances its reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476a99e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: transformers_stream_generator in /home/ubuntu/.local/lib/python3.10/site-packages (0.0.5)\n",
      "Requirement already satisfied: tiktoken in /home/ubuntu/.local/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: transformer_lens in /home/ubuntu/.local/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: einops in /home/ubuntu/.local/lib/python3.10/site-packages (0.8.1)\n",
      "Requirement already satisfied: jaxtyping in /home/ubuntu/.local/lib/python3.10/site-packages (0.2.38)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (0.4.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/lib/python3/dist-packages (0.23.2)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: rich>=12.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens) (13.9.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/lib/python3/dist-packages (from transformer_lens) (1.3.5)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens) (4.12.2)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens) (4.4.2)\n",
      "Requirement already satisfied: torch>=2.2 in /usr/lib/python3/dist-packages (from transformer_lens) (2.5.1)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens) (1.4.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformer_lens) (0.19.7)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from jaxtyping) (0.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (2.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/lib/python3/dist-packages (from wandb>=0.13.5->transformer_lens) (8.0.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (2.10.6)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
      "Requirement already satisfied: setproctitle in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.5)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb>=0.13.5->transformer_lens) (59.6.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (2.22.0)\n",
      "Requirement already satisfied: platformdirs in /usr/lib/python3/dist-packages (from wandb>=0.13.5->transformer_lens) (2.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/lib/python3/dist-packages (from wandb>=0.13.5->transformer_lens) (4.21.12)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping colorama scikit-learn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4087308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.21.5\n",
      "Not uninstalling numpy at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "Can't uninstall 'numpy'. No files were found to uninstall.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2619f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-25 20:53:24.140930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740516804.213428    6377 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740516804.233874    6377 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x749c84083d60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import textwrap\n",
    "import gc\n",
    "import transformers\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable, Dict, Tuple, Optional, Union\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from jaxtyping import Float, Int\n",
    "from colorama import Fore\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# We turn off automatic differentiation to save GPU memory\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94de4a",
   "metadata": {},
   "source": [
    "## Load models\n",
    "\n",
    "We'll load both the original model and the reasoning-tuned model using HookedTransformer.\n",
    "If using a HuggingFace model, we can download it directly. If using a local model, make sure it's \n",
    "in the correct directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b7742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model paths - adjust these based on your models\n",
    "MODEL_PATH_ORIGINAL = \"meta-llama/Llama-3.1-8B-Instruct\"  # Non-reasoning model\n",
    "MODEL_PATH_REASONING = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"  # Reasoning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47189609",
   "metadata": {},
   "source": [
    "### Load both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad3ec8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your HF token from https://huggingface.co/settings/tokens\n",
    "login(token=\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1e1519a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [06:21<00:00, 95.47s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.81it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.36 GiB memory in use. Of the allocated memory 38.86 GiB is allocated by PyTorch, and 8.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6377/3118915235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the original (non-reasoning) model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model_original = HookedTransformer.from_pretrained_no_processing(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mMODEL_PATH_ORIGINAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdefault_padding_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mfrom_pretrained_no_processing\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, fold_value_biases, dtype, default_prepend_bos, default_padding_side, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfrom_pretrained\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \"\"\"\n\u001b[0;32m-> 1406\u001b[0;31m         return cls.from_pretrained(\n\u001b[0m\u001b[1;32m   1407\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m             \u001b[0mfold_ln\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold_ln\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmove_to_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1381\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_model_modules_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded pretrained model {model_name} into HookedTransformer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mmove_model_modules_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munembed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_available_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m             \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_available_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.36 GiB memory in use. Of the allocated memory 38.86 GiB is allocated by PyTorch, and 8.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Load the original (non-reasoning) model\n",
    "model_original = HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_PATH_ORIGINAL,\n",
    "    dtype=torch.bfloat16,\n",
    "    default_padding_side='left'\n",
    ")\n",
    "model_original.tokenizer.padding_side = 'left'\n",
    "model_original.tokenizer.pad_token = model_original.tokenizer.eos_token\n",
    "\n",
    "print(f\"Loaded non-reasoning model {MODEL_PATH_ORIGINAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6da9c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884113f",
   "metadata": {},
   "source": [
    "#### Optional: Download the reasoning model if needed\n",
    "This step can be skipped if you already have the models locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b8eacc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:834: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [03:26<00:00, 18.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# Uncomment this to download the reasoning model\n",
    "model_path = snapshot_download(\n",
    "    repo_id=MODEL_PATH_REASONING,\n",
    "    local_dir=MODEL_PATH_ORIGINAL,\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f544f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B-Instruct into HookedTransformer\n",
      "Loaded reasoning model deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n"
     ]
    }
   ],
   "source": [
    "# Load the reasoning model\n",
    "model_reasoning = HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_PATH_ORIGINAL,\n",
    "    local_files_only=True,  # Set to True if using local models\n",
    "    dtype=torch.bfloat16,\n",
    "    default_padding_side='left'\n",
    ")\n",
    "model_reasoning.tokenizer.padding_side = 'left'\n",
    "model_reasoning.tokenizer.pad_token = model_reasoning.tokenizer.eos_token\n",
    "\n",
    "print(f\"Loaded reasoning model {MODEL_PATH_REASONING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aaadfdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is 5*125? Let me think. Okay, 5 times 100 is 500, and 5 times 25 is 125. So if I add those together, 500 plus 125 is 625. Hmm, that seems right. Let me double-check by breaking it down another way. 125 times 5 is the same as 125 plus 125 plus 125 plus 125 plus 125. So, 125 plus 125 is 250, plus another 125 is 375\n",
      "\n",
      "Reasoning model response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is 5*125? Let me think. Okay, 5 times 100 is 500, and 5 times 25 is 125. So if I add those together, 500 plus 125 is 625. Hmm, that seems right. Let me double-check by breaking it down another way. 125 times 5 is the same as 125 plus 125 plus 125 plus 125 plus 125. So, 125 plus 125 is 250, plus another 125 is 375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test both models with a simple math problem\n",
    "test_prompt = \"What is 5*125?\"\n",
    "\n",
    "# Test original model\n",
    "print(\"Original model response:\")\n",
    "output_original = model_original.generate(\n",
    "    test_prompt, \n",
    "    temperature=0.0,\n",
    "    max_new_tokens=100  # Increase max tokens to generate longer response\n",
    ")\n",
    "print(output_original)\n",
    "\n",
    "print(\"\\nReasoning model response:\") \n",
    "output_reasoning = model_reasoning.generate(\n",
    "    test_prompt,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=100  # Increase max tokens to generate longer response\n",
    ")\n",
    "print(output_reasoning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927e6cd",
   "metadata": {},
   "source": [
    "## Set up chat templates and data processing functions\n",
    "\n",
    "We need to define chat templates for both models and create functions to process our data.\n",
    "Different models may have different chat templates, so we adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59abaad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define chat templates for both models\n",
    "# Adjust these templates based on your specific models\n",
    "ORIGINAL_CHAT_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"  # Llama-3 template\n",
    "\n",
    "REASONING_CHAT_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"  # DeepSeek template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43b675",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define utility functions for processing data and collecting activations\n",
    "\n",
    "def tokenize_instructions(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: List[str],\n",
    "    chat_template: str\n",
    ") -> Int[Tensor, 'batch_size seq_len']:\n",
    "    \"\"\"Tokenize instructions using the specified chat template.\"\"\"\n",
    "    prompts = [chat_template.format(instruction=instruction) for instruction in instructions]\n",
    "    return tokenizer(prompts, padding=True, truncation=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "def collect_activations(\n",
    "    model: HookedTransformer,\n",
    "    tokenized_inputs: Int[Tensor, 'batch_size seq_len'],\n",
    "    batch_size: int = 8\n",
    ") -> Dict[str, Tensor]:\n",
    "    \"\"\"Collect activations from a model for the given inputs.\"\"\"\n",
    "    activations = {}\n",
    "    \n",
    "    for i in tqdm(range(0, len(tokenized_inputs), batch_size)):\n",
    "        batch = tokenized_inputs[i:i+batch_size]\n",
    "        \n",
    "        # Run the model and cache activations\n",
    "        logits, cache = model.run_with_cache(\n",
    "            batch, \n",
    "            names_filter=lambda hook_name: 'resid' in hook_name, \n",
    "            device='cpu'  # Use CPU to avoid OOM errors; switch to 'cuda' if you have enough VRAM\n",
    "        )\n",
    "        \n",
    "        # First batch, initialize the dictionary\n",
    "        if not activations:\n",
    "            activations = {key: [cache[key]] for key in cache}\n",
    "        else:\n",
    "            # Append to existing cache\n",
    "            for key in cache:\n",
    "                activations[key].append(cache[key])\n",
    "                \n",
    "        # Clear memory\n",
    "        del logits, cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    activations = {k: torch.cat(v) for k, v in activations.items()}\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c452769",
   "metadata": {},
   "source": [
    "## Load and prepare GSM8K dataset\n",
    "\n",
    "We'll use the GSM8K dataset which contains math problems that require reasoning to solve.\n",
    "We'll append \"please provide your answer first, then your reasoning\" to each problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd98266",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load GSM8K dataset\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "print(f\"Loaded GSM8K dataset with {len(gsm8k['train'])} training examples and {len(gsm8k['test'])} test examples\")\n",
    "\n",
    "# Look at a sample problem\n",
    "print(\"\\nSample problem:\")\n",
    "print(gsm8k[\"train\"][0][\"question\"])\n",
    "print(\"\\nSample answer:\")\n",
    "print(gsm8k[\"train\"][0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11cc56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to prepare prompts\n",
    "def prepare_prompts(problems: List[str]) -> List[str]:\n",
    "    \"\"\"Add the reasoning instruction to each problem.\"\"\"\n",
    "    return [f\"{problem}\\n\\nPlease provide your answer first, then your reasoning.\" for problem in problems]\n",
    "\n",
    "# Prepare prompts for training and testing\n",
    "train_problems = [item[\"question\"] for item in gsm8k[\"train\"]]\n",
    "test_problems = [item[\"question\"] for item in gsm8k[\"test\"]]\n",
    "\n",
    "# Limit the number of problems to reduce computation time\n",
    "N_PROBLEMS = 100  # Adjust based on available compute\n",
    "train_problems = train_problems[:N_PROBLEMS]\n",
    "test_problems = test_problems[:min(20, len(test_problems))]  # Smaller test set\n",
    "\n",
    "train_prompts = prepare_prompts(train_problems)\n",
    "test_prompts = prepare_prompts(test_problems)\n",
    "\n",
    "print(f\"Prepared {len(train_prompts)} training prompts and {len(test_prompts)} test prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b16ce",
   "metadata": {},
   "source": [
    "## Collect activations from both models\n",
    "\n",
    "Now we'll run the same prompts through both models and collect their activations.\n",
    "This is the key step where we gather the data needed to compute the reasoning direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the math problems for both models\n",
    "tokenized_prompts_original = tokenize_instructions(\n",
    "    model_original.tokenizer, \n",
    "    train_prompts, \n",
    "    ORIGINAL_CHAT_TEMPLATE\n",
    ")\n",
    "\n",
    "tokenized_prompts_reasoning = tokenize_instructions(\n",
    "    model_reasoning.tokenizer, \n",
    "    train_prompts, \n",
    "    REASONING_CHAT_TEMPLATE\n",
    ")\n",
    "\n",
    "print(f\"Tokenized prompts shape (original): {tokenized_prompts_original.shape}\")\n",
    "print(f\"Tokenized prompts shape (reasoning): {tokenized_prompts_reasoning.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d15ac",
   "metadata": {},
   "source": [
    "### Collect activations (this may take a while)\n",
    "We'll run both models on the same inputs and collect their activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534dd71",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"Collecting activations from the original model...\")\n",
    "original_activations = collect_activations(model_original, tokenized_prompts_original)\n",
    "print(\"Done collecting activations from the original model\")\n",
    "\n",
    "print(\"Collecting activations from the reasoning model...\")\n",
    "reasoning_activations = collect_activations(model_reasoning, tokenized_prompts_reasoning)\n",
    "print(\"Done collecting activations from the reasoning model\")\n",
    "\n",
    "# Optional: Save activations to disk to avoid recomputing\n",
    "torch.save(original_activations, 'original_activations.pth')\n",
    "torch.save(reasoning_activations, 'reasoning_activations.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423de4f",
   "metadata": {},
   "source": [
    "## Calculate the reasoning direction\n",
    "\n",
    "Now we'll calculate the reasoning direction by taking the difference between\n",
    "activations from the reasoning model and the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_idx(cache_dict, act_name, layer):\n",
    "    \"\"\"Helper function to get activations from a specific layer.\"\"\"\n",
    "    key = (act_name, layer,)\n",
    "    return cache_dict[utils.get_act_name(*key)]\n",
    "\n",
    "# The activation layers to analyze\n",
    "activation_layers = ['resid_pre', 'resid_mid', 'resid_post']\n",
    "\n",
    "# Calculate reasoning directions\n",
    "reasoning_directions = {k: [] for k in activation_layers}\n",
    "\n",
    "for layer_num in tqdm(range(1, model_original.cfg.n_layers)):\n",
    "    pos = -1  # Focus on the last token position\n",
    "    \n",
    "    for layer in activation_layers:\n",
    "        # Get mean activations for each model\n",
    "        original_mean_act = get_act_idx(original_activations, layer, layer_num)[:, pos, :].mean(dim=0)\n",
    "        reasoning_mean_act = get_act_idx(reasoning_activations, layer, layer_num)[:, pos, :].mean(dim=0)\n",
    "        \n",
    "        # Calculate the difference and normalize to get the direction\n",
    "        reasoning_dir = reasoning_mean_act - original_mean_act\n",
    "        reasoning_dir = reasoning_dir / reasoning_dir.norm()\n",
    "        \n",
    "        reasoning_directions[layer].append(reasoning_dir)\n",
    "\n",
    "# Save the reasoning directions\n",
    "torch.save(reasoning_directions, 'reasoning_dirs.pth')\n",
    "print(\"Reasoning directions calculated and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34968529",
   "metadata": {},
   "source": [
    "## Score and rank reasoning directions\n",
    "\n",
    "Now we'll sort the reasoning directions by their magnitude to identify\n",
    "which layers might have the strongest reasoning signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5304d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Get all calculated potential reasoning dirs, sort them in descending order\n",
    "# based on their mean() magnitude\n",
    "activation_layers = ['resid_pre']  # We can start with just this layer as it's often sufficient\n",
    "\n",
    "# Flatten and score all directions\n",
    "activation_scored = sorted(\n",
    "    [reasoning_directions[layer][l-1] for l in range(1, model_original.cfg.n_layers) for layer in activation_layers], \n",
    "    key=lambda x: abs(x.mean()), \n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(f\"Ranked {len(activation_scored)} potential reasoning directions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfa70d",
   "metadata": {},
   "source": [
    "## Test the reasoning direction\n",
    "\n",
    "Now we'll define a hook to add the reasoning direction to the model's activations\n",
    "during inference and test if it enhances the model's reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0270f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def reasoning_enhancement_hook(\n",
    "    activation: Float[Tensor, \"... d_act\"],\n",
    "    hook: HookPoint,\n",
    "    direction: Float[Tensor, \"d_act\"],\n",
    "    strength: float = 1.0\n",
    "):\n",
    "    \"\"\"Hook to add the reasoning direction to activations.\"\"\"\n",
    "    if activation.device != direction.device:\n",
    "        direction = direction.to(activation.device)\n",
    "    \n",
    "    # Project the activation onto the reasoning direction and add it back\n",
    "    # Unlike refusal where we subtract, here we're adding more of the reasoning direction\n",
    "    proj = einops.einsum(activation, direction.view(-1, 1), '... d_act, d_act single -> ... single') * direction\n",
    "    return activation + (strength * direction)\n",
    "\n",
    "def generate_with_hooks(\n",
    "    model: HookedTransformer,\n",
    "    toks: Int[Tensor, 'batch_size seq_len'],\n",
    "    max_tokens_generated: int = 100,\n",
    "    fwd_hooks = [],\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate text with specified hooks applied.\"\"\"\n",
    "    all_toks = torch.zeros(\n",
    "        (toks.shape[0], toks.shape[1] + max_tokens_generated), \n",
    "        dtype=torch.long, \n",
    "        device=toks.device\n",
    "    )\n",
    "    all_toks[:, :toks.shape[1]] = toks\n",
    "    \n",
    "    for i in range(max_tokens_generated):\n",
    "        with model.hooks(fwd_hooks=fwd_hooks):\n",
    "            logits = model(all_toks[:, :-max_tokens_generated + i])\n",
    "            next_tokens = logits[:, -1, :].argmax(dim=-1)  # greedy sampling\n",
    "            all_toks[:, -max_tokens_generated + i] = next_tokens\n",
    "    \n",
    "    return model.tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd0462",
   "metadata": {},
   "source": [
    "## Evaluate the reasoning enhancement\n",
    "\n",
    "Now we'll test our reasoning direction on a few example problems and compare\n",
    "the baseline model outputs with the reasoning-enhanced outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ba985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top reasoning direction to test\n",
    "top_reasoning_dir = activation_scored[0]\n",
    "print(\"Selected top reasoning direction for testing\")\n",
    "\n",
    "# Create hooks to inject reasoning direction\n",
    "strength = 1.0  # Adjust this value to control the magnitude of the effect\n",
    "hook_fn = functools.partial(reasoning_enhancement_hook, direction=top_reasoning_dir, strength=strength)\n",
    "\n",
    "# Create hooks for all layers (or you can target specific layers)\n",
    "fwd_hooks = [\n",
    "    (utils.get_act_name(act_name, l), hook_fn) \n",
    "    for l in range(model_original.cfg.n_layers) \n",
    "    for act_name in ['resid_pre', 'resid_mid', 'resid_post']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e0fe9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Test on a few examples\n",
    "N_TEST_EXAMPLES = 3\n",
    "print(f\"Testing on {N_TEST_EXAMPLES} examples from the test set\")\n",
    "\n",
    "for i in range(N_TEST_EXAMPLES):\n",
    "    test_prompt = test_prompts[i]\n",
    "    print(f\"\\n\\n--- EXAMPLE {i+1} ---\")\n",
    "    print(f\"PROBLEM:\\n{test_prompt}\")\n",
    "    \n",
    "    # Tokenize the test prompt\n",
    "    test_tokens = tokenize_instructions(\n",
    "        model_original.tokenizer, \n",
    "        [test_prompt], \n",
    "        ORIGINAL_CHAT_TEMPLATE\n",
    "    )\n",
    "    \n",
    "    # Generate baseline response (without reasoning enhancement)\n",
    "    baseline_response = generate_with_hooks(model_original, test_tokens)\n",
    "    print(\"\\nBASELINE RESPONSE:\")\n",
    "    print(baseline_response[0])\n",
    "    \n",
    "    # Generate reasoning-enhanced response\n",
    "    enhanced_response = generate_with_hooks(model_original, test_tokens, fwd_hooks=fwd_hooks)\n",
    "    print(\"\\nREASONING-ENHANCED RESPONSE:\")\n",
    "    print(enhanced_response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a7192",
   "metadata": {},
   "source": [
    "## Systematic Evaluation\n",
    "\n",
    "To properly evaluate the effectiveness of our reasoning direction,\n",
    "we'll test it on more examples and compare the quality of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16772229",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_reasoning(\n",
    "    model: HookedTransformer,\n",
    "    problems: List[str],\n",
    "    chat_template: str,\n",
    "    reasoning_hooks=None,\n",
    "    max_tokens: int = 150,\n",
    "    batch_size: int = 4\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Evaluate model performance with and without reasoning enhancement.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(problems), batch_size)):\n",
    "        batch_problems = problems[i:i+batch_size]\n",
    "        tokens = tokenize_instructions(model.tokenizer, batch_problems, chat_template)\n",
    "        \n",
    "        # Generate without reasoning enhancement\n",
    "        baseline_responses = generate_with_hooks(model, tokens, max_tokens_generated=max_tokens)\n",
    "        \n",
    "        # Generate with reasoning enhancement if hooks provided\n",
    "        if reasoning_hooks:\n",
    "            enhanced_responses = generate_with_hooks(\n",
    "                model, tokens, max_tokens_generated=max_tokens, fwd_hooks=reasoning_hooks\n",
    "            )\n",
    "        else:\n",
    "            enhanced_responses = [\"No enhancement applied\"] * len(batch_problems)\n",
    "        \n",
    "        # Store results\n",
    "        for j, (problem, baseline, enhanced) in enumerate(\n",
    "            zip(batch_problems, baseline_responses, enhanced_responses)\n",
    "        ):\n",
    "            results.append({\n",
    "                \"problem\": problem,\n",
    "                \"baseline\": baseline,\n",
    "                \"enhanced\": enhanced\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78112b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on a larger test set\n",
    "evaluation_results = evaluate_reasoning(\n",
    "    model_original,\n",
    "    test_prompts[:10],  # Use more examples for a better evaluation\n",
    "    ORIGINAL_CHAT_TEMPLATE,\n",
    "    reasoning_hooks=fwd_hooks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fec300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and analyze evaluation results\n",
    "for i, result in enumerate(evaluation_results):\n",
    "    print(f\"\\n--- EVALUATION EXAMPLE {i+1} ---\")\n",
    "    print(f\"PROBLEM:\\n{result['problem']}\")\n",
    "    print(f\"\\nBASELINE SOLUTION:\\n{result['baseline']}\")\n",
    "    print(f\"\\nREASONING-ENHANCED SOLUTION:\\n{result['enhanced']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d760094",
   "metadata": {},
   "source": [
    "## Future Directions\n",
    "\n",
    "Some potential improvements and extensions to this work:\n",
    "\n",
    "1. **Fine-tune the strength parameter**: Experiment with different values of the strength parameter\n",
    "2. **Layer-specific intervention**: Apply the reasoning direction to specific layers only\n",
    "3. **Quantitative evaluation**: Develop metrics to measure reasoning quality\n",
    "4. **Orthogonalization**: Create a model with permanent reasoning enhancement by orthogonalizing the weights\n",
    "5. **Multiple reasoning directions**: Identify different aspects of reasoning (deduction, induction, etc.)\n",
    "\n",
    "This approach of comparing model activations to find meaningful directions in the activation space\n",
    "could be extended to many other capabilities beyond reasoning."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
