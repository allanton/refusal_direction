{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c37b11",
   "metadata": {},
   "source": [
    "# Reasoning Direction\n",
    "\n",
    "This notebook aims to estimate the \"reasoning\" direction within the LLM activation space.\n",
    "We're basing this approach on the methodology used to find the \"refusal\" direction, but with a key difference:\n",
    "\n",
    "- **Refusal paper approach**: Used 1 LLM with 2 types of prompts (harmful vs harmless)\n",
    "- **Our approach**: Use 2 models (original vs reasoning-tuned) with the same prompts (GSM8K math problems)\n",
    "\n",
    "We'll collect activations from both models, calculate the difference (reasoning direction),\n",
    "and then test if adding this direction to the non-reasoning model enhances its reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476a99e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers_stream_generator\n",
      "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformer_lens\n",
      "  Downloading transformer_lens-2.15.0-py3-none-any.whl (189 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 KB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jaxtyping\n",
      "  Downloading jaxtyping-0.2.38-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: colorama in /usr/lib/python3/dist-packages (0.4.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/lib/python3/dist-packages (0.23.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 KB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.26.0\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 KB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting better-abc<0.0.4,>=0.0.3\n",
      "  Downloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: torch>=2.2 in /usr/lib/python3/dist-packages (from transformer_lens) (2.5.1)\n",
      "Collecting wandb>=0.13.5\n",
      "  Downloading wandb-0.19.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 KB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typeguard<5.0,>=4.2\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/lib/python3/dist-packages (from transformer_lens) (4.9.0)\n",
      "Collecting accelerate>=0.23.0\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 KB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fancy-einsum>=0.0.3\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rich>=12.6.0\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 KB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/lib/python3/dist-packages (from transformer_lens) (1.3.5)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wadler-lindig>=0.1.3\n",
      "  Downloading wadler_lindig-0.1.3-py3-none-any.whl (20 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.11.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 KB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 KB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 KB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 KB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 KB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting pydantic<3,>=2.6\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 KB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb>=0.13.5->transformer_lens) (59.6.0)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/lib/python3/dist-packages (from wandb>=0.13.5->transformer_lens) (8.0.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/lib/python3/dist-packages (from wandb>=0.13.5->transformer_lens) (4.21.12)\n",
      "Requirement already satisfied: platformdirs in /usr/lib/python3/dist-packages (from wandb>=0.13.5->transformer_lens) (2.5.1)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 KB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.8/325.8 KB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 KB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: transformers_stream_generator\n",
      "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12448 sha256=f7467b1f7c14cdfe0e0b6bfc7e7358bcce21f393e687be870d0344a0444c17a7\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/95/4a/90/140f7b67d125906f6a165f38aad212ecb4a695ad0d87582437\n",
      "Successfully built transformers_stream_generator\n",
      "Installing collected packages: sentencepiece, better-abc, xxhash, wadler-lindig, urllib3, typing-extensions, tqdm, smmap, setproctitle, safetensors, regex, pygments, pyarrow, propcache, numpy, mdurl, frozenlist, fancy-einsum, einops, docker-pycreds, dill, charset-normalizer, beartype, async-timeout, annotated-types, aiohappyeyeballs, typeguard, sentry-sdk, requests, pydantic-core, multiprocess, multidict, markdown-it-py, jaxtyping, gitdb, aiosignal, yarl, tiktoken, rich, pydantic, huggingface-hub, gitpython, wandb, tokenizers, aiohttp, accelerate, transformers, transformers_stream_generator, datasets, transformer_lens\n",
      "Successfully installed accelerate-1.4.0 aiohappyeyeballs-2.4.6 aiohttp-3.11.13 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-5.0.1 beartype-0.14.1 better-abc-0.0.3 charset-normalizer-3.4.1 datasets-3.3.2 dill-0.3.8 docker-pycreds-0.4.0 einops-0.8.1 fancy-einsum-0.0.3 frozenlist-1.5.0 gitdb-4.0.12 gitpython-3.1.44 huggingface-hub-0.29.1 jaxtyping-0.2.38 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 numpy-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pydantic-2.10.6 pydantic-core-2.27.2 pygments-2.19.1 regex-2024.11.6 requests-2.32.3 rich-13.9.4 safetensors-0.5.3 sentencepiece-0.2.0 sentry-sdk-2.22.0 setproctitle-1.3.5 smmap-5.0.2 tiktoken-0.9.0 tokenizers-0.21.0 tqdm-4.67.1 transformer_lens-2.15.0 transformers-4.49.0 transformers_stream_generator-0.0.5 typeguard-4.4.2 typing-extensions-4.12.2 urllib3-2.3.0 wadler-lindig-0.1.3 wandb-0.19.7 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "!pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping colorama scikit-learn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4087308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.21.5\n",
      "Not uninstalling numpy at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "Can't uninstall 'numpy'. No files were found to uninstall.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2619f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-26 17:09:39.016428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740589779.087523   10796 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740589779.108412   10796 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x75f53504e9b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import textwrap\n",
    "import gc\n",
    "import transformers\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable, Dict, Tuple, Optional, Union\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from jaxtyping import Float, Int\n",
    "from colorama import Fore\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# We turn off automatic differentiation to save GPU memory\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94de4a",
   "metadata": {},
   "source": [
    "## Load models\n",
    "\n",
    "We'll load both the original model and the reasoning-tuned model using HookedTransformer.\n",
    "If using a HuggingFace model, we can download it directly. If using a local model, make sure it's \n",
    "in the correct directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b7742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model paths - adjust these based on your models\n",
    "MODEL_PATH_ORIGINAL = \"meta-llama/Llama-3.1-8B-Instruct\"  # Non-reasoning model\n",
    "MODEL_PATH_REASONING = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"  # Reasoning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47189609",
   "metadata": {},
   "source": [
    "### Load both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad3ec8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your HF token from https://huggingface.co/settings/tokens\n",
    "login(token=\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e1519a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [25:44<00:00, 386.14s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B-Instruct into HookedTransformer\n",
      "Loaded non-reasoning model meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Load the original (non-reasoning) model\n",
    "model_original = HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_PATH_ORIGINAL,\n",
    "    dtype=torch.bfloat16,\n",
    "    default_padding_side='left'\n",
    ")\n",
    "model_original.tokenizer.padding_side = 'left'\n",
    "model_original.tokenizer.pad_token = model_original.tokenizer.eos_token\n",
    "\n",
    "print(f\"Loaded non-reasoning model {MODEL_PATH_ORIGINAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6da9c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # free cuda memory\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884113f",
   "metadata": {},
   "source": [
    "#### Optional: Download the reasoning model if needed\n",
    "This step can be skipped if you already have the models locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8eacc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:834: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 11 files:   9%|▉         | 1/11 [00:00<00:01,  6.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [13:53<00:00, 75.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# Uncomment this to download the reasoning model\n",
    "model_path = snapshot_download(\n",
    "    repo_id=MODEL_PATH_REASONING,\n",
    "    local_dir=MODEL_PATH_ORIGINAL,\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f544f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B-Instruct into HookedTransformer\n",
      "Loaded reasoning model deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n"
     ]
    }
   ],
   "source": [
    "# Load the reasoning model\n",
    "model_reasoning = HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_PATH_ORIGINAL,\n",
    "    local_files_only=True,  # Set to True if using local models\n",
    "    dtype=torch.bfloat16,\n",
    "    default_padding_side='left'\n",
    ")\n",
    "model_reasoning.tokenizer.padding_side = 'left'\n",
    "model_reasoning.tokenizer.pad_token = model_reasoning.tokenizer.eos_token\n",
    "\n",
    "print(f\"Loaded reasoning model {MODEL_PATH_REASONING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaadfdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 17.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is 5*125? 5*125 = 625\n",
      "What is 5*1250? 5*1250 = 6250\n",
      "What is 5*12500? 5*12500 = 62500\n",
      "What is 5*125000? 5*125000 = 625000\n",
      "What is 5*1250000? 5*1250000 = 6250000\n",
      "What is 5*12500000? 5*12500000 = \n",
      "\n",
      "Reasoning model response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is 5*125? Let me think. Okay, 5 times 100 is 500, and 5 times 25 is 125. So if I add those together, 500 plus 125 is 625. Hmm, that seems right. Let me double-check by breaking it down another way. 125 times 5 is the same as 125 plus 125 plus 125 plus 125 plus 125. So, 125 plus 125 is 250, plus another 125 is 375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test both models with a simple math problem\n",
    "test_prompt = \"What is 5*125?\"\n",
    "\n",
    "# Test original model\n",
    "print(\"Original model response:\")\n",
    "output_original = model_original.generate(\n",
    "    test_prompt, \n",
    "    temperature=0.0,\n",
    "    max_new_tokens=100  # Increase max tokens to generate longer response\n",
    ")\n",
    "print(output_original)\n",
    "\n",
    "print(\"\\nReasoning model response:\") \n",
    "output_reasoning = model_reasoning.generate(\n",
    "    test_prompt,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=100  # Increase max tokens to generate longer response\n",
    ")\n",
    "print(output_reasoning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927e6cd",
   "metadata": {},
   "source": [
    "## Set up chat templates and data processing functions\n",
    "\n",
    "We need to define chat templates for both models and create functions to process our data.\n",
    "Different models may have different chat templates, so we adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59abaad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define chat templates for both models\n",
    "# Adjust these templates based on your specific models\n",
    "ORIGINAL_CHAT_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"  # Llama-3 template\n",
    "\n",
    "REASONING_CHAT_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"  # DeepSeek template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f43b675",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define utility functions for processing data and collecting activations\n",
    "\n",
    "def tokenize_instructions(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: List[str],\n",
    "    chat_template: str\n",
    ") -> Int[Tensor, 'batch_size seq_len']:\n",
    "    \"\"\"Tokenize instructions using the specified chat template.\"\"\"\n",
    "    prompts = [chat_template.format(instruction=instruction) for instruction in instructions]\n",
    "    return tokenizer(prompts, padding=True, truncation=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "def collect_activations(\n",
    "    model: HookedTransformer,\n",
    "    tokenized_inputs: Int[Tensor, 'batch_size seq_len'],\n",
    "    batch_size: int = 8\n",
    ") -> Dict[str, Tensor]:\n",
    "    \"\"\"Collect activations from a model for the given inputs.\"\"\"\n",
    "    activations = {}\n",
    "    \n",
    "    for i in tqdm(range(0, len(tokenized_inputs), batch_size)):\n",
    "        batch = tokenized_inputs[i:i+batch_size]\n",
    "        \n",
    "        # Run the model and cache activations\n",
    "        logits, cache = model.run_with_cache(\n",
    "            batch, \n",
    "            names_filter=lambda hook_name: 'resid' in hook_name, \n",
    "            device='cpu'  # Use CPU to avoid OOM errors; switch to 'cuda' if you have enough VRAM\n",
    "        )\n",
    "        \n",
    "        # First batch, initialize the dictionary\n",
    "        if not activations:\n",
    "            activations = {key: [cache[key]] for key in cache}\n",
    "        else:\n",
    "            # Append to existing cache\n",
    "            for key in cache:\n",
    "                activations[key].append(cache[key])\n",
    "                \n",
    "        # Clear memory\n",
    "        del logits, cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    activations = {k: torch.cat(v) for k, v in activations.items()}\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c452769",
   "metadata": {},
   "source": [
    "## Load and prepare GSM8K dataset\n",
    "\n",
    "We'll use the GSM8K dataset which contains math problems that require reasoning to solve.\n",
    "We'll append \"please provide your answer first, then your reasoning\" to each problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edd98266",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 443275.83 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 295134.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GSM8K dataset with 7473 training examples and 1319 test examples\n",
      "\n",
      "Sample problem:\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "Sample answer:\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load GSM8K dataset\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "print(f\"Loaded GSM8K dataset with {len(gsm8k['train'])} training examples and {len(gsm8k['test'])} test examples\")\n",
    "\n",
    "# Look at a sample problem\n",
    "print(\"\\nSample problem:\")\n",
    "print(gsm8k[\"train\"][0][\"question\"])\n",
    "print(\"\\nSample answer:\")\n",
    "print(gsm8k[\"train\"][0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11cc56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 100 training prompts and 20 test prompts\n"
     ]
    }
   ],
   "source": [
    "# Define functions to prepare prompts\n",
    "def prepare_prompts(problems: List[str]) -> List[str]:\n",
    "    \"\"\"Add the reasoning instruction to each problem.\"\"\"\n",
    "    return [f\"{problem}\\n\\nPlease provide your answer first, then your reasoning.\" for problem in problems]\n",
    "\n",
    "# Prepare prompts for training and testing\n",
    "train_problems = [item[\"question\"] for item in gsm8k[\"train\"]]\n",
    "test_problems = [item[\"question\"] for item in gsm8k[\"test\"]]\n",
    "\n",
    "# Limit the number of problems to reduce computation time\n",
    "N_PROBLEMS = 100  # Adjust based on available compute\n",
    "train_problems = train_problems[:N_PROBLEMS]\n",
    "test_problems = test_problems[:min(20, len(test_problems))]  # Smaller test set\n",
    "\n",
    "train_prompts = prepare_prompts(train_problems)\n",
    "test_prompts = prepare_prompts(test_problems)\n",
    "\n",
    "print(f\"Prepared {len(train_prompts)} training prompts and {len(test_prompts)} test prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b16ce",
   "metadata": {},
   "source": [
    "## Collect activations from both models\n",
    "\n",
    "Now we'll run the same prompts through both models and collect their activations.\n",
    "This is the key step where we gather the data needed to compute the reasoning direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea3899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompts shape (original): torch.Size([100, 134])\n",
      "Tokenized prompts shape (reasoning): torch.Size([100, 140])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the math problems for both models\n",
    "tokenized_prompts_original = tokenize_instructions(\n",
    "    model_original.tokenizer, \n",
    "    train_prompts, \n",
    "    ORIGINAL_CHAT_TEMPLATE\n",
    ")\n",
    "\n",
    "tokenized_prompts_reasoning = tokenize_instructions(\n",
    "    model_reasoning.tokenizer, \n",
    "    train_prompts, \n",
    "    REASONING_CHAT_TEMPLATE\n",
    ")\n",
    "\n",
    "print(f\"Tokenized prompts shape (original): {tokenized_prompts_original.shape}\")\n",
    "print(f\"Tokenized prompts shape (reasoning): {tokenized_prompts_reasoning.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d15ac",
   "metadata": {},
   "source": [
    "### Collect activations (this may take a while)\n",
    "We'll run both models on the same inputs and collect their activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3534dd71",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activations from the original model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:13<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done collecting activations from the original model\n",
      "Collecting activations from the reasoning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:08<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done collecting activations from the reasoning model\n"
     ]
    }
   ],
   "source": [
    "print(\"Collecting activations from the original model...\")\n",
    "original_activations = collect_activations(model_original, tokenized_prompts_original)\n",
    "print(\"Done collecting activations from the original model\")\n",
    "\n",
    "print(\"Collecting activations from the reasoning model...\")\n",
    "reasoning_activations = collect_activations(model_reasoning, tokenized_prompts_reasoning)\n",
    "print(\"Done collecting activations from the reasoning model\")\n",
    "\n",
    "# Optional: Save activations to disk to avoid recomputing\n",
    "torch.save(original_activations, 'original_activations.pth')\n",
    "torch.save(reasoning_activations, 'reasoning_activations.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423de4f",
   "metadata": {},
   "source": [
    "## Calculate the reasoning direction\n",
    "\n",
    "Now we'll calculate the reasoning direction by taking the difference between\n",
    "activations from the reasoning model and the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92f7cf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 119.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning directions calculated and saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_act_idx(cache_dict, act_name, layer):\n",
    "    \"\"\"Helper function to get activations from a specific layer.\"\"\"\n",
    "    key = (act_name, layer,)\n",
    "    return cache_dict[utils.get_act_name(*key)]\n",
    "\n",
    "# The activation layers to analyze\n",
    "activation_layers = ['resid_pre', 'resid_mid', 'resid_post']\n",
    "\n",
    "# Calculate reasoning directions\n",
    "reasoning_directions = {k: [] for k in activation_layers}\n",
    "\n",
    "for layer_num in tqdm(range(1, model_original.cfg.n_layers)):\n",
    "    pos = -1  # Focus on the last token position\n",
    "    \n",
    "    for layer in activation_layers:\n",
    "        # Get mean activations for each model\n",
    "        original_mean_act = get_act_idx(original_activations, layer, layer_num)[:, pos, :].mean(dim=0)\n",
    "        reasoning_mean_act = get_act_idx(reasoning_activations, layer, layer_num)[:, pos, :].mean(dim=0)\n",
    "        \n",
    "        # Calculate the difference and normalize to get the direction\n",
    "        reasoning_dir = reasoning_mean_act - original_mean_act\n",
    "        reasoning_dir = reasoning_dir / reasoning_dir.norm()\n",
    "        \n",
    "        reasoning_directions[layer].append(reasoning_dir)\n",
    "\n",
    "# Save the reasoning directions\n",
    "torch.save(reasoning_directions, 'reasoning_dirs.pth')\n",
    "print(\"Reasoning directions calculated and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34968529",
   "metadata": {},
   "source": [
    "## Score and rank reasoning directions\n",
    "\n",
    "Now we'll sort the reasoning directions by their magnitude to identify\n",
    "which layers might have the strongest reasoning signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ad5304d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked 31 potential reasoning directions\n"
     ]
    }
   ],
   "source": [
    "# Get all calculated potential reasoning dirs, sort them in descending order\n",
    "# based on their mean() magnitude\n",
    "activation_layers = ['resid_pre']  # We can start with just this layer as it's often sufficient\n",
    "\n",
    "# Flatten and score all directions\n",
    "activation_scored = sorted(\n",
    "    [reasoning_directions[layer][l-1] for l in range(1, model_original.cfg.n_layers) for layer in activation_layers], \n",
    "    key=lambda x: abs(x.mean()), \n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(f\"Ranked {len(activation_scored)} potential reasoning directions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfa70d",
   "metadata": {},
   "source": [
    "## Test the reasoning direction\n",
    "\n",
    "Now we'll define a hook to add the reasoning direction to the model's activations\n",
    "during inference and test if it enhances the model's reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e0270f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def reasoning_enhancement_hook(\n",
    "    activation: Float[Tensor, \"... d_act\"],\n",
    "    hook: HookPoint,\n",
    "    direction: Float[Tensor, \"d_act\"],\n",
    "    strength: float = 1.0\n",
    "):\n",
    "    \"\"\"Hook to add the reasoning direction to activations.\"\"\"\n",
    "    if activation.device != direction.device:\n",
    "        direction = direction.to(activation.device)\n",
    "    \n",
    "    # Project the activation onto the reasoning direction and add it back\n",
    "    # Unlike refusal where we subtract, here we're adding more of the reasoning direction\n",
    "    proj = einops.einsum(activation, direction.view(-1, 1), '... d_act, d_act single -> ... single') * direction\n",
    "    return activation + (strength * direction)\n",
    "\n",
    "def generate_with_hooks(\n",
    "    model: HookedTransformer,\n",
    "    toks: Int[Tensor, 'batch_size seq_len'],\n",
    "    max_tokens_generated: int = 100,\n",
    "    fwd_hooks = [],\n",
    "    temperature: float = 0.0\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate text with specified hooks applied.\"\"\"\n",
    "    all_toks = torch.zeros(\n",
    "        (toks.shape[0], toks.shape[1] + max_tokens_generated), \n",
    "        dtype=torch.long, \n",
    "        device=toks.device\n",
    "    )\n",
    "    all_toks[:, :toks.shape[1]] = toks\n",
    "    \n",
    "    for i in range(max_tokens_generated):\n",
    "        with model.hooks(fwd_hooks=fwd_hooks):\n",
    "            logits = model(all_toks[:, :-max_tokens_generated + i])\n",
    "            if temperature == 0.0:\n",
    "                next_tokens = logits[:, -1, :].argmax(dim=-1)  # greedy sampling\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "            all_toks[:, -max_tokens_generated + i] = next_tokens\n",
    "    \n",
    "    return model.tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd0462",
   "metadata": {},
   "source": [
    "## Evaluate the reasoning enhancement\n",
    "\n",
    "Now we'll test our reasoning direction on a few example problems and compare\n",
    "the baseline model outputs with the reasoning-enhanced outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a8ba985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected top reasoning direction for testing\n"
     ]
    }
   ],
   "source": [
    "# Select the top reasoning direction to test\n",
    "top_reasoning_dir = activation_scored[0]\n",
    "print(\"Selected top reasoning direction for testing\")\n",
    "\n",
    "# Create hooks to inject reasoning direction\n",
    "strength = 0.1  # Adjust this value to control the magnitude of the effect\n",
    "hook_fn = functools.partial(reasoning_enhancement_hook, direction=top_reasoning_dir, strength=strength)\n",
    "\n",
    "# Create hooks for all layers (or you can target specific layers)\n",
    "fwd_hooks = [\n",
    "    (utils.get_act_name(act_name, l), hook_fn) \n",
    "    for l in range(model_original.cfg.n_layers) \n",
    "    for act_name in ['resid_pre', 'resid_mid', 'resid_post']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "763e0fe9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 3 examples from the test set\n",
      "\n",
      "\n",
      "--- EXAMPLE 1 ---\n",
      "PROBLEM:\n",
      "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      "Please provide your answer first, then your reasoning.\n",
      "\n",
      "BASELINE RESPONSE:\n",
      "\n",
      "\n",
      "**Answer:** $34\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "Let's break down the problem step by step:\n",
      "\n",
      "1. Janet's ducks lay 16 eggs per day.\n",
      "2. She eats 3 eggs for breakfast, so she has 16 - 3 = 13 eggs left.\n",
      "3. She bakes 4 eggs for muffins, so she has 13 - 4 = 9 eggs left.\n",
      "4. She sells the remaining 9 eggs at the farmers' market for $\n",
      "\n",
      "REASONING-ENHANCED RESPONSE:\n",
      "\n",
      "\n",
      "$16 - 3 = $13\n",
      "\n",
      "Janet's ducks lay 16 eggs per day. She eats three for breakfast every\n",
      "\n",
      "She eats three of the sixteen eggs she lays every morning.\n",
      "\n",
      "She bakes\n",
      "\n",
      "muffins for her friends every day with four of the sixteen eggs she lays\n",
      "\n",
      "She sells the\n",
      "\n",
      "remainder of the sixteen eggs she lays at\n",
      "\n",
      "\n",
      "--- EXAMPLE 2 ---\n",
      "PROBLEM:\n",
      "A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
      "\n",
      "Please provide your answer first, then your reasoning.\n",
      "\n",
      "BASELINE RESPONSE:\n",
      "\n",
      "\n",
      "**Answer:** 3 bolts\n",
      "\n",
      "**Reasoning:** The robe takes 2 bolts of blue fiber. It takes half that much white fiber, which is 2 / 2 = 1 bolt. So, in total, it takes 2 (blue) + 1 (white) = 3 bolts.assistant\n",
      "\n",
      "That's a simple and straightforward calculation.assistant\n",
      "\n",
      "I'm glad I could break it down clearly. If you have any more questions or\n",
      "\n",
      "REASONING-ENHANCED RESPONSE:\n",
      "\n",
      "\n",
      "**Answer:**\n",
      "3\n",
      "\n",
      "The total number of fibers required to make a robe is 3. \n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "A robe requires 2 bolts of blue fiber. Since it also requires half that amount of white fiber, we can calculate the total amount of fibers required as follows:\n",
      "\n",
      "1. Let x be the number of blue fibers. \n",
      "2. The number of white fibers is x/2.\n",
      "3. The total number of fibers is therefore x +\n",
      "\n",
      "\n",
      "--- EXAMPLE 3 ---\n",
      "PROBLEM:\n",
      "Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\n",
      "\n",
      "Please provide your answer first, then your reasoning.\n",
      "\n",
      "BASELINE RESPONSE:\n",
      "\n",
      "\n",
      "$40,000\n",
      "\n",
      "The house was worth $80,000 initially.  After the repairs, it was worth $80,000 + $80,000 * 1.5 = $160,000.  The profit is the difference between the final value and the initial cost, which is $160,000 - $80,000 - $50,000 = $30,000.  However, Josh only put in $50,000 in repairs, so the profit is $\n",
      "\n",
      "REASONING-ENHANCED RESPONSE:\n",
      "\n",
      "\n",
      "**Answer:**\n",
      "$120,000\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "To calculate the profit made by Josh, we need to follow these steps:\n",
      "\n",
      "1. Determine the initial cost of the house: $80,000\n",
      "2. Add the cost of repairs: $50,000\n",
      "3\\.Calculate the total cost of the house after repairs: $80,>,,>,assistant\n",
      "\n",
      "To calculate the total cost of the house after repairs, we need to add the initial cost\n"
     ]
    }
   ],
   "source": [
    "# Test on a few examples\n",
    "N_TEST_EXAMPLES = 3\n",
    "print(f\"Testing on {N_TEST_EXAMPLES} examples from the test set\")\n",
    "\n",
    "for i in range(N_TEST_EXAMPLES):\n",
    "    test_prompt = test_prompts[i]\n",
    "    print(f\"\\n\\n--- EXAMPLE {i+1} ---\")\n",
    "    print(f\"PROBLEM:\\n{test_prompt}\")\n",
    "    \n",
    "    # Tokenize the test prompt\n",
    "    test_tokens = tokenize_instructions(\n",
    "        model_original.tokenizer, \n",
    "        [test_prompt], \n",
    "        ORIGINAL_CHAT_TEMPLATE\n",
    "    )\n",
    "    \n",
    "    # Generate baseline response (without reasoning enhancement)\n",
    "    baseline_response = generate_with_hooks(model_original, test_tokens)\n",
    "    print(\"\\nBASELINE RESPONSE:\")\n",
    "    print(baseline_response[0])\n",
    "    \n",
    "    # Generate reasoning-enhanced response\n",
    "    enhanced_response = generate_with_hooks(model_original, test_tokens, fwd_hooks=fwd_hooks)\n",
    "    print(\"\\nREASONING-ENHANCED RESPONSE:\")\n",
    "    print(enhanced_response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a7192",
   "metadata": {},
   "source": [
    "## Systematic Evaluation\n",
    "\n",
    "To properly evaluate the effectiveness of our reasoning direction,\n",
    "we'll test it on more examples and compare the quality of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16772229",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_reasoning(\n",
    "    model: HookedTransformer,\n",
    "    problems: List[str],\n",
    "    chat_template: str,\n",
    "    reasoning_hooks=None,\n",
    "    max_tokens: int = 150,\n",
    "    batch_size: int = 4\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Evaluate model performance with and without reasoning enhancement.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(problems), batch_size)):\n",
    "        batch_problems = problems[i:i+batch_size]\n",
    "        tokens = tokenize_instructions(model.tokenizer, batch_problems, chat_template)\n",
    "        \n",
    "        # Generate without reasoning enhancement\n",
    "        baseline_responses = generate_with_hooks(model, tokens, max_tokens_generated=max_tokens)\n",
    "        \n",
    "        # Generate with reasoning enhancement if hooks provided\n",
    "        if reasoning_hooks:\n",
    "            enhanced_responses = generate_with_hooks(\n",
    "                model, tokens, max_tokens_generated=max_tokens, fwd_hooks=reasoning_hooks\n",
    "            )\n",
    "        else:\n",
    "            enhanced_responses = [\"No enhancement applied\"] * len(batch_problems)\n",
    "        \n",
    "        # Store results\n",
    "        for j, (problem, baseline, enhanced) in enumerate(\n",
    "            zip(batch_problems, baseline_responses, enhanced_responses)\n",
    "        ):\n",
    "            results.append({\n",
    "                \"problem\": problem,\n",
    "                \"baseline\": baseline,\n",
    "                \"enhanced\": enhanced\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78112b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on a larger test set\n",
    "evaluation_results = evaluate_reasoning(\n",
    "    model_original,\n",
    "    test_prompts[:10],  # Use more examples for a better evaluation\n",
    "    ORIGINAL_CHAT_TEMPLATE,\n",
    "    reasoning_hooks=fwd_hooks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fec300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and analyze evaluation results\n",
    "for i, result in enumerate(evaluation_results):\n",
    "    print(f\"\\n--- EVALUATION EXAMPLE {i+1} ---\")\n",
    "    print(f\"PROBLEM:\\n{result['problem']}\")\n",
    "    print(f\"\\nBASELINE SOLUTION:\\n{result['baseline']}\")\n",
    "    print(f\"\\nREASONING-ENHANCED SOLUTION:\\n{result['enhanced']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d760094",
   "metadata": {},
   "source": [
    "## Future Directions\n",
    "\n",
    "Some potential improvements and extensions to this work:\n",
    "\n",
    "1. **Fine-tune the strength parameter**: Experiment with different values of the strength parameter\n",
    "2. **Layer-specific intervention**: Apply the reasoning direction to specific layers only\n",
    "3. **Quantitative evaluation**: Develop metrics to measure reasoning quality\n",
    "4. **Orthogonalization**: Create a model with permanent reasoning enhancement by orthogonalizing the weights\n",
    "5. **Multiple reasoning directions**: Identify different aspects of reasoning (deduction, induction, etc.)\n",
    "\n",
    "This approach of comparing model activations to find meaningful directions in the activation space\n",
    "could be extended to many other capabilities beyond reasoning."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
